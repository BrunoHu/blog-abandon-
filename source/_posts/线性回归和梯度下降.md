title: 线性回归和梯度下降
date: 2016-06-04 23:20:31
mathjax: true
tags:
- machine learing
- math
- algorithm
---

找了工作之后好像好久没有更新了,很多东西都忘记了,刚好最近正在重新入门machine learning,就写一下最简单的线性回归,重新捡一下快忘记的numpy, matplotlib,　和latex.

现在假设我们有$ｍ$个数据$x$,并且每个数据都有$ｎ$个特征,相应的这些数据所对应的结果为$ｙ$,也是$ｍ$个.我们希望能找到一组线性参数来对任意一个样本尽量准的预测其结果.这样的参数向量我们称为$\theta$,而我们希望用这样一个函数来预测$y$
$$y = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + \ldots + \theta_n x_n$$
或者用简洁的矩阵表示
$$y = \theta^Tx$$
而梯度下降法则是提供了一个有效的学习方法,从每次训练得到的反馈中修正参数直到收敛.

那根据上面的定义,我们就能很明确在这个过程中有两件事很关键.

一是如何度量预测的准不准

二是如何根据每次的反馈修正参数


度量预测

通过$\theta$和$x$,我们可以预测出对应的$y$,然而这个预测出的$ｙ$和真实的$ｙ$肯定是有误差的,那么自然而然,我们在借用统计里的方法,可以定义这个准确程度为
$$J(\theta) = \sum_{i=1}^m \left (h_{\theta}(x^{i}) - y^{(i)} \right )^2$$

现在我们有了一个可以度量误差的方法,那么,我们的目标也就显而易见了,就是使这个误差函数最小.而如何使其最小则是梯度下降法的工作了.

当我们得到一个函数,我们怎么能够知道往哪个方向走下一步能够最快的接近最小值呢？对高等数学熟悉的话一下子就能想到梯度的概念---在函数上的任意一点,其导数的方向就是函数值变化最大的方向.那么我们可以通过求导来获得下一步的方向.

$$\begin{align}
\frac{\partial J(\theta)}{\partial \theta_i} &=  \frac{\partial}{\partial \theta_i} \sum_{i=1}^m \left (h_{\theta}(x^{i}) - y^{(i)} \right )^2 \\
&=  2\sum_{i=1}^m \left ( (h_{\theta}(x^{i}) - y^{(i)}) \, \frac{\partial}{\partial \theta_i}(h_{\theta}(x^{i}) - y^{(i)}) \right ) \\
&= 2\sum_{i=1}^m \left( h_{\theta}(x^{i}) - y^{(i)}\right) x_i
\end{align}
$$
在很多参考书中,为了整洁美观,都会使$J(\theta)$前多加一个参数$\frac{1}{2}$,使得导数变成$\sum_{i=1}^m \left( h_{\theta}(x^{i}) - y^{(i)}\right) x_i$,我们随大流,也用这样的方法定义,不过其实都没有差啦.


由此我们得到每一次的迭代方法
$$\theta_i := \theta_i - \alpha\sum_{i=1}^m \left( h_{\theta}(x^{i}) - y^{(i)}\right) x_i \tag{ for i=1,2 \ldots ,n }$$
如果我们用更简洁耳朵矩阵表示则是
$$\theta := \theta - \alpha\left( sum(\theta^T x - y)x\right)$$

这里的$\alpha$是每次梯度下降的步长,步长如果选短了会下降的很慢,迭代很多次,二步长选长了则会出现在最优点附近徘徊甚至直接远离最优点.选好$\alpha$真是一门学问.

通过这种方法迭代的梯度下降法称为批量梯度爱心将法,因为每一次迭代都需要所有的训练点参与计算.二还有另外一种增量梯度下降算法或者说最忌梯度下降算法则每一次只使用一个点来训练,他的迭代公式是
$$\theta := \theta - \alpha \left( \theta^T x^{(i)} - y{(i)} \right)x$$

好的,下面我们就要上图了.


